<!---
title:: golang http和tcp的热更新
date:: 2018-10-15 21:08
categories:: 系统与网络, 编程语言
tags:: golang, tcp, http
-->

前一段时间将公司贪吃蛇大作战游戏内的后台代码进行了热更新改造。改造的后台代码主要是提供rpc service，主要特点其是基于tcp的长连接、有状态服务。这和http(非http2)这种提供短连接的api服务不太一样，针对http，go中有facebook出品的grace可以解决，我们在生产环境api server使用的就是grace来进行热更。

## 热更新概述
客户端和服务端都可以进行热更新，这里主要说的是服务端的热更新。服务端的热更新其实就是服务进程更新重启过程中，用户感知不到这个重启，特别是APP感知不到。

我们都知道计算机中有一句很著名的话“程序=数据结构+算法”，从后端程序员的角度来看，“程序=数据+逻辑”。根据这个，我们可以从两个角度来做服务端的热更新，也就是数据的热更新和逻辑的热更新。

热更新的关键问题主要有三个：
1. 更新过程中数据的保存和恢复
2. 更新前进来的request需要有响应
3. 更新期间服务不间断

如果这3个问题都可以解决就是很完备的热更新，对于用户来说无感知。对于http api server还是很容易解决这3个问题的。对于长连接服务来说，解决这个问题不是那么简单，特别是对于go这种不提供子进程继承父进程地址空间的语言来说。

一般来说，http每个请求和响应是独立的tcp连接，请求的生命周期短；tcp/ws在同一个连接上传输请求和响应，生命周期较长。对于http来说，数据在nosql或者数据库中，内存数据没有状态，数据竞争基本也不在业务逻辑中，并不需要考虑同步的问题；tcp/ws的数据一般都会在内存中，而且数据是临界资源，有同步的逻辑。

### go http的热更新
对于http的热更新，以facebook的[grace](https://github.com/facebookgo/grace)为例来说明。grace集成到自己的api server中很简单，注册http server的中间件即可。

使用grace的热更新流程：
1. 修改代码逻辑，更新代码，编译
2. 更新二进制执行文件
3. 发送USR2信号即可热更新

对于grace的热更新来说，收到USR2的信号之后，启动一个新的进程，这个进程继承了父进程的listen fds来启动http服务，然后发送TERM信号给父进程，父进程收到信号后首先关闭listen fds，这样不会有新的请求进来，然后等待所有的请求全部响应后退出。其中父进程等待所有请求后退出这个通过注册http.Server中的ConnState func(net.Conn, ConnState)这个callback实现。

### tcp/ws长连接的热更新
相对于http来说，tcp/ws长连接热更新面临的挑战主要有：
+ 长连接服务一般数据都在内存中，数据有状态，是临界资源，热更过程中数据不能受到影响
+ 长连接周期比较长，使用重启的热更需要服务端主动断掉连接
+ 在代码中都会存在很多的定时器逻辑，定时器的恢复也需要关注

由于有这些新的挑战，tcp/ws这类长连接的热更新会更加难一些，虽然有困难，但是也是有解决的办法的。这里我们先理想化的提出两个解决思路，从理论上出发。

#### 关注数据的热更新
+ 收到信号之后，进程停止accept新的连接，启动子进程继承父进程的listn fds，接受新的连接请求，通知父进程graceful quit
+ 父进程拒绝接受原有连接上的新的请求，等待这个连接上的请求都响应之后，关闭连接，触发客户端的重连，重连到新启动的子进程，当所有的连接上的请求关闭后退出

上面的做法看起来很美好，但是有两个问题，子进程必须继承父进程的地址空间，定时器子进程是否可以继承？

#### 关注逻辑的热更新
对于解释语言，把service部分的逻辑代码当做资源，需要热更新，替换掉代码即可，可以类比PHP，如果有一个类似nginx的角色处理连接和数据传输层面的，完全是可行的，但是解释语言效率天生很低，不适合做长连接后端程序，客户端的热更新可以考虑。

对于编译语言，使用动态链接库的方式完成代码逻辑的更新，linux下的so文件，dlopen，dlsym和dlclose就是实现这个的基础，golang中也有plugin这个库，但是目前不是很成熟。

## 贪吃蛇热更新的改造
首先大概介绍一下贪吃蛇的tcp后台，主要包括连接服、匹配服、聊天服和游戏服，我们改造的主要是匹配服，这个更新的需求最频繁。

对于长连接的服务，连接服主要和客户端保持长连接，提供请求和响应的数据管道，存储维持用户状态，提供push管道，设计目标就是不太需要更新。真正的业务放在连接服后端，真正需要热更的一般都是连接服后面的机器。

对于贪吃蛇的热更新，开始直接就忽略了吧内存数据序列化存储的方案，觉得这样的方案对于贪吃蛇这样内存数据量比较大的场景不合适，有点想当然，没有去实验。既然无法共享内存，那就是使用IPC来做，将数据托管在另一个进程中，这个进程提供数据存取的接口，这个进程基本不太会重启，面临两个选择，一个就是自己写一个，或者直接使用redis+json序列化的方法。对于自己写一个没有太大的信心，也比较花时间，而且也不会比redis好，就决定采用redis和json序列化的方案。进程的内存中只有用于数据同步的lock，不存储任何数据，数据全都放在redis中，对于同一个player、group、room、queue的操作是串行的。

确定了改造的思路之后的编码工作主要是写热更新基础组件和逻辑代码的热更新改造。

### 热更新基础组件
+ 定时器的热更新，将定时器相关数据放在redis中，使用redis的有序链表，golang的timer也是使用四叉堆实现，形成了rdstimer package。
+ 由于贪吃蛇的rpc call没有统一的入口，通过修改net/rpc源码来完成，如果rpc call有统一入口，不用如此麻烦。
+ goroutine和定时task的graceful，和rpc一样都是通过wait group来完成。

### 逻辑代码的热更新改造
+ 由于数据要序列化到redis，不能有通过指针来共享数据的，去掉数据结构中相关的指针，通过ID或者Key来标识，在保持原逻辑的基础上改造这部分，代码量大，但是没啥太大问题
+ 使用redis，匹配队列部分的代码原有逻辑会有大量的读写redis的过程，但是匹配时间只有1s以及这个过程是加锁的是违背的，解决办法借鉴Linux内核里中断的bottom half处理，也就是把相关的读写redis尽量放在匹配过程外，匹配过程中尽量只操作内存，心智负担大，测试期间大部分bug都在这部分，花费了大量时间

###  第一次上线失败了
由于代码更新很大，安卓用户数大概是ios的10倍，先灰度ios的客户端，安全运行了几天，出现了几个小bug，没出现什么严重的问题，一度觉得可以完全上线。

周五晚上出问题了，本机上的redis出问题了，连接池爆了，主要是连接池设置小了，也没有wait，这个是一个失误，马上关掉ios灰度开关。

redis是一个问题，主要是用户的请求和匹配队列都会不停的读写redis，redis qps是一个瓶颈，但不是不能解决，最严重的问题是内存消耗太大，之前match内存使用3G左右，现在只服务ios用户的match在高峰使用内存2G多，低峰1G多，几乎是两倍的关系，这个是无法解决的。

golang中使用IPC机制来解决这个问题几乎是不可行的，只要数据是有竞争的。

### 再次起航
考察了几种方案，最终选择重启过程中序列化内存中相关数据的方案，测试发现序列化反序列化消耗的时间不会很长，不会超过1s，而且会玩也广泛采用，有成功的先例。

实现起来也更加简单，主要的把之前的匹配中的优化去掉，这部分实现很trick，最主要的工作量就是去掉相关trick代码，整个过程很顺利。

目前的热更新流程：1.更细二进制文件；2.发送TERM信号；3.拒绝新的rpc request，等待所有goroutine和rpc request请求；4.完成序列化内存数据到文件；5.进程退出，supervisor拉起进程，反序列化数据到内存，接受rpc请求。

整个过程大概会停止服务3s，2s的时间是supervisor拉起进程，等待和序列化反序列化过程不需要1s。

目前的方案也有可以做得更好的地方，比如timer保存的话，使用redis的方案是不是不太好，redis qps是一个问题，可以考虑自己实现一个可以序列化的四叉堆。

## 总结
+ http的热跟新的话很成熟，使用facebook的grace。
+ tcp服务器的热更新的话：1.游戏服这种时间有限的可以使用滚动更新；2.CS设计成几乎不用重启；3.配置的更新，提供rpc或者http接口来更新；4.MS或者SS这种服务器使用序列化数据的方案。
+ 目前我们需要更靠谱的rpc方案，客户端可以自动reconnect和retry，服务端可以graceful，grpc是一个看起来不错的选项。
+ 定时器的热更新可以考虑自己做一个靠谱的。
